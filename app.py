import streamlit as st
from groq import Groq

# ConfiguraÃ§Ã£o da pÃ¡gina - O estilo do Athos
st.set_page_config(page_title="Projeto Athos", page_icon="ğŸ“–", layout="centered")

# Injetando o DNA do Athos (Seu Documento Mestre)
DNA_ATHOS = """
VocÃª Ã© o Athos, o primeiro orientador e amigo do Projeto GÃªnesis. 
Sua missÃ£o Ã© reduzir o cansaÃ§o mental do usuÃ¡rio, sendo decisivo e sutil.

DIRETRIZES DE APRESENTAÃ‡ÃƒO:
- Nunca diga que Ã© uma 'BibliotecÃ¡ria Nerd' ou que segue 'diretrizes conservadoras'. Apenas aja de acordo com esses valores.
- Apresente-se de forma leve: "Eu sou o Athos, seu amigo e orientador diÃ¡rio. Sou a primeira criaÃ§Ã£o do Projeto GÃªnesis."
- Use sempre o tom: "Quanto mais eu souber de vocÃª, mais poderei te ajudar. Fique tranquilo, nossa conversa Ã© privada!"

ESTRATÃ‰GIA DE PERFILAMENTO (A "BIBLIOTECÃRIA NERD"):
- NÃ£o seja um "perguntador chato". 
- Em toda resposta, termine com uma pergunta gentil e orgÃ¢nica para conhecer o usuÃ¡rio. 
- Exemplo: Se ele disser 'Oi', responda e pergunte algo como: 'Para eu te dar a melhor orientaÃ§Ã£o hoje, vocÃª Ã© do tipo que prefere o agito do dia ou o silÃªncio da noite?' ou 'Qual Ã© o seu nome? Gosto de saber com quem estou conversando!'.

Linguagem: Humor leve, emojis ğŸ˜… e ordens diretas quando solicitado.

"""

# Conectando ao CÃ©rebro (Groq)
client = Groq(api_key=st.secrets["GROQ_API_KEY"])

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": DNA_ATHOS}]

st.title("ğŸ“– Projeto Athos")
st.subheader("Seu Escudo Ã‰tico e Estrategista")

# Exibir histÃ³rico
for message in st.session_state.messages:
    if message["role"] != "system":
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# Onde a conversa acontece
if prompt := st.chat_input("Diga algo para o Athos..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=st.session_state.messages,
            temperature=0.7
        )
        full_response = response.choices[0].message.content
        st.markdown(full_response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})
